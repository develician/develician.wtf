import styles from "./document.module.css";
import pyplotImg from "./pyplot.png";

<div>

# 선형 회귀 모델 구현하기

## 선형회귀란?

### 선형 회귀: Linear Regression

- x 와 y값을 가지고 서로간의 관계를 파악하는것.
- 어떠한 입력에 대해서 출력값을 예측하는것.
- 머신러닝의 기본.
- 지도학습(Supervised Learning) 예측 문제에 사용하는 알고리즘.
- 선형회귀는 기본적으로 설명변수와 반응변수가 연속형 변수일 때 사용할수 있다.
- 만약 설명변수가 범주형 변수인 경우, 이를 더미변수 (Dummy Variable)로 변환하여 회귀분석을 적용해야 한다.

### 손실 함수: Loss Function

- 한 쌍(x, y)의 데이터에 대한 손실값을 계산하는 함수.
- 손실값이란 실제값과 모델로 예측한 값이 얼마나 차이가 나는가 이다.
- 즉 손실값이 작을수록 그 모델이 X 와 Y의 관계를 잘 설명하고 있다는 것.
- 이 손실을 전체 데이터에 대해 구한 경우 이를 비용(cost)이라고 한다.

손실 함수는 '예측값과 실제값의 거리'를 가장 많이 사용한다.
따라서 손실값은 예측값에서 실제값을 뺀뒤 제곱하여, 모든 데이터에 대한 평균을 내어 구한다.

이 손실함수를 만들때에는 잔차의 개념이 도입된다.

### 잔차: Residual

- 잔차란 관측값의 y와 예측값의 y 간의 차이를 말한다.
- 잔차 = hypothesis - y (여기서 hypothesis = W \* X + b, y는 예측값.)

```
A(1, 4) B(2, 3) 2개의점이 있다. 회귀식 = 2x + 1
A의 관측값 y는 4 예측값 y는 3
B의 관측값 y는 3, 예측값 y는 5

이때 A의 잔차는 4 - 3 = 1
B의 잔차는 3 - 5 = -2 이다
```

### 최소 제곱법: method of least squares, least squares approximation

최소 제곱법은 잔차의 제곱의 합이 최소가 되도록 하는 직선을 회귀선으로 한다는 것을 의미.

```
(hypothesis - 예측값) ** 2 -> 최소로 한다.
위에서 최소 제곱법으로 나온 값은 = (1) ** 2 + (-2) ** 2 = 5.
```

### 최적화 함수

- 가중치(W: Weight)와 편향(b: bias)값을 변경해가면서 손실값을 최소화 하는 가장 최적화된 가중치와 편향값을 찾아주는 함수.
- 값들을 무작위로 변경할시, 시간이 너무 오래걸리고, 학습 시간도 예측하기 어려울것.
- 따라서 빠르게 최적화 하기위한 다양한 방법 사용.

### 경사하강법: 최적화 방식중 하나

- 경사하강법은 최적화 방법중 가장 기본적인 알고리즘 으로, 함수의 기울기르루 구하고, 기울기가 낮은쪽으로 계속 이동시키면서 최적의 값을 찾아가는 방법
- Learning Rate, 즉 학습률은 학습을 얼마나 급하게 할것인가를 설정하는 값.
- 러닝 레이트가 너무 크면 최적의 손실갑슬 찾지 못하고 지나치게 되고, 너무 작으면 학습 속도가 매우 느려진다.

### 하이퍼 파라미터: hyperparameter

- 위의 Learning Rate처럼 학습에 영향을 주는 변수를 하이퍼 파라미터 라고 한다.
- 이 값에 따라 학습속도나 신경망 성능이 크게 달라진다.

`linear-regression-tf.py`

```python
# $ conda install tensorflow
# $ conda install matplotlib

import tensorflow as tf
import matplotlib.pyplot as plt

x_data = [1, 2, 3]
y_data = [2, 4, 6]

# 위와 같은 두배의 값을 규칙으로 하는 값들이 있다고 하자.

# y = W * X + b
# 출력값 = 가중치 * 입력값 + 편향

W = tf.Variable(tf.random_normal([1], -1., 1.))
b = tf.Variable(tf.random_normal([1], -1., 1.))

# 가중치와 편향 값을 랜덤하게 하나를 뽑는 작업.

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

# x_data와 y_data의 값을 담을 플레이스홀더를 선언.

hypothesis = W * X + b

# 위의 식을 바탕으로 hypothesis 라는 식 선언
# X와 W가 행렬이 아니므로 tf.matmul이 아니라 기본 곱셈 연산자를 사용.

# 손실함수 작성 : 잔차의 최소 제곱법을 적용한후 모든 데이터에 대한 손실값의 평균을 내어 구한것.
# 잔차 = hypothesis - y
# tf.square(잔차) -> 잔차의 최소 제곱법을 적용
# tf.reduce_mean(tf.square(잔차)) -> 잔차의 최소 제곱법을 적용한후 모든 데이터에 대한 손실값의 평균을 내어 구한것.
cost = tf.reduce_mean(tf.square(hypothesis - Y))

# 경사 하강법 최적화 함수를 이용해 손실값을 최소화 하는 연산 그래프 생성.
# 최적화 함수
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op = optimizer.minimize(cost)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(100):
        _, cost_val = sess.run([train_op, cost], feed_dict={
                               X: x_data, Y: y_data})

        print(step, cost_val, sess.run(W), sess.run(b))
        # 여기서 찍힌 값을 보면, step이 늘어날수록, 손실값은 작아지는 것을 볼수 있다.

    prediction_X_4 = sess.run(hypothesis, feed_dict={X: 4})
    prediction_X_5 = sess.run(hypothesis, feed_dict={X: 5})
    print("X: 5 일때 예측값 Y:", prediction_X_4)
    print("X: 10 일때 예측값 Y:", prediction_X_5)

    plt.figure(1)
    plt.title('Linear Regression')
    plt.xlabel('입력값')
    plt.ylabel('출력값')
    # x_data와 y_data를 기준으로 점을 찍는다.
    plt.plot(x_data, y_data, 'ro')
    # 예측한 일차함수를 선으로 표시
    plt.plot(x_data, sess.run(W) * x_data + sess.run(b), 'b')
    # x = 5 일때 초록점 찍기
    plt.plot([4], prediction_X_4, 'go')
    # x = 10 일때 초록점 찍기
    plt.plot([5], prediction_X_5, 'go')

```

결과

```
0 42.470684 [2.4284465] [-0.22302985]
1 0.52415997 [2.1177752] [-0.3498025]
2 0.022300875 [2.1477726] [-0.3269521]
3 0.015544206 [2.1406324] [-0.3206707]
4 0.014737797 [2.1376438] [-0.3127895]
5 0.014036932 [2.1342921] [-0.30528915]
6 0.013370161 [2.1310685] [-0.29794815]
7 0.01273507 [2.127917] [-0.2907859]
8 0.012130133 [2.1248422] [-0.28379554]
9 0.01155397 [2.121841] [-0.2769733]
10 0.011005126 [2.118912] [-0.27031505]
11 0.010482387 [2.1160536] [-0.26381683]
12 0.009984459 [2.1132636] [-0.2574749]
13 0.009510189 [2.1105409] [-0.25128537]
14 0.009058433 [2.1078835] [-0.24524464]
15 0.00862817 [2.1052902] [-0.2393491]
16 0.008218323 [2.102759] [-0.23359534]
17 0.007827944 [2.1002886] [-0.22797982]
18 0.007456124 [2.0978777] [-0.22249934]
19 0.007101948 [2.0955248] [-0.21715058]
20 0.006764582 [2.0932286] [-0.21193036]
21 0.0064432784 [2.0909872] [-0.20683575]
22 0.0061372 [2.0888002] [-0.20186348]
23 0.0058456827 [2.0866654] [-0.19701086]
24 0.0055680103 [2.084582] [-0.19227485]
25 0.005303502 [2.0825489] [-0.18765269]
26 0.0050515966 [2.0805643] [-0.1831417]
27 0.0048116376 [2.0786276] [-0.17873904]
28 0.0045830896 [2.0767374] [-0.17444226]
29 0.0043653883 [2.0748928] [-0.17024876]
30 0.0041580102 [2.0730925] [-0.16615608]
31 0.0039605093 [2.0713353] [-0.16216184]
32 0.0037723917 [2.0696204] [-0.15826361]
33 0.0035932038 [2.0679467] [-0.15445904]
34 0.0034225248 [2.0663133] [-0.15074593]
35 0.003259943 [2.0647192] [-0.14712206]
36 0.003105099 [2.0631635] [-0.14358532]
37 0.0029575967 [2.061645] [-0.14013366]
38 0.0028171048 [2.0601633] [-0.13676494]
39 0.0026832952 [2.058717] [-0.13347723]
40 0.002555849 [2.0573053] [-0.1302686]
41 0.0024344374 [2.0559278] [-0.12713702]
42 0.0023187972 [2.0545833] [-0.12408071]
43 0.0022086473 [2.0532713] [-0.12109789]
44 0.0021037406 [2.0519905] [-0.11818682]
45 0.0020038097 [2.0507407] [-0.11534566]
46 0.0019086379 [2.049521] [-0.11257282]
47 0.0018179723 [2.0483305] [-0.10986665]
48 0.0017316194 [2.0471687] [-0.10722556]
49 0.0016493658 [2.0460348] [-0.10464794]
50 0.0015710144 [2.0449283] [-0.10213227]
51 0.0014963896 [2.0438483] [-0.09967712]
52 0.001425319 [2.042794] [-0.09728102]
53 0.0013576051 [2.0417655] [-0.09494241]
54 0.0012931268 [2.0407612] [-0.09266011]
55 0.0012317062 [2.0397813] [-0.09043261]
56 0.0011731881 [2.0388253] [-0.08825859]
57 0.0011174673 [2.0378919] [-0.086137]
58 0.0010643912 [2.0369809] [-0.08406635]
59 0.0010138314 [2.0360918] [-0.08204543]
60 0.00096566504 [2.0352242] [-0.08007307]
61 0.00091980613 [2.0343773] [-0.07814816]
62 0.00087610446 [2.033551] [-0.07626946]
63 0.0008344829 [2.0327446] [-0.07443593]
64 0.00079485815 [2.0319571] [-0.07264663]
65 0.000757093 [2.0311892] [-0.07090017]
66 0.00072113913 [2.0304394] [-0.06919584]
67 0.0006868792 [2.0297077] [-0.06753244]
68 0.00065424625 [2.0289936] [-0.06590898]
69 0.00062317564 [2.0282965] [-0.06432464]
70 0.0005935691 [2.0276163] [-0.06277829]
71 0.00056536996 [2.0269525] [-0.06126911]
72 0.0005385172 [2.0263045] [-0.05979627]
73 0.0005129425 [2.0256722] [-0.05835882]
74 0.0004885812 [2.025055] [-0.05695596]
75 0.0004653704 [2.0244527] [-0.05558674]
76 0.0004432618 [2.023865] [-0.05425046]
77 0.00042221425 [2.023291] [-0.05294639]
78 0.00040214974 [2.0227313] [-0.05167355]
79 0.0003830522 [2.0221848] [-0.05043137]
80 0.00036485636 [2.0216515] [-0.04921905]
81 0.0003475259 [2.021131] [-0.04803585]
82 0.00033101955 [2.020623] [-0.04688111]
83 0.0003152896 [2.0201273] [-0.04575405]
84 0.00030031707 [2.0196433] [-0.04465417]
85 0.00028604764 [2.0191712] [-0.04358065]
86 0.0002724634 [2.0187104] [-0.04253303]
87 0.00025952377 [2.0182605] [-0.0415106]
88 0.00024719306 [2.0178216] [-0.04051268]
89 0.00023545395 [2.017393] [-0.0395388]
90 0.00022426603 [2.0169752] [-0.03858828]
91 0.00021361478 [2.016567] [-0.03766069]
92 0.00020346681 [2.0161688] [-0.03675534]
93 0.00019380315 [2.01578] [-0.03587181]
94 0.0001845964 [2.0154006] [-0.03500943]
95 0.0001758262 [2.0150306] [-0.0341678]
96 0.00016747485 [2.0146692] [-0.03334647]
97 0.00015951767 [2.0143166] [-0.03254483]
98 0.00015194587 [2.0139723] [-0.0317625]
99 0.00014472575 [2.0136366] [-0.0309989]
X: 5 일때 예측값 Y: [8.023547]
X: 10 일때 예측값 Y: [10.037184]

```

<img src={pyplotImg} className={styles["ImageStyle"]} />
</div>
